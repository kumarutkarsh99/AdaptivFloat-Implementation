{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a307713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import core pytorch + computer vision libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from functools import partial\n",
    "\n",
    "# import transformer model + tokenizer + dataset utilities\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# import custom modules\n",
    "import quantizer\n",
    "import utils\n",
    "\n",
    "# set device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a89cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load base BERT model + tokenizer for binary classification\n",
    "model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model_bert_fp32_clf = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "\n",
    "# load GLUE SST-2 validation split\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "sst2_val = dataset['validation']\n",
    "\n",
    "# tokenizer helper for SST-2 sentences\n",
    "def tokenize_sst2(batch):\n",
    "    return bert_tokenizer(batch['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "# map tokenizer over entire validation set\n",
    "sst2_val = sst2_val.map(tokenize_sst2, batched=True)\n",
    "\n",
    "# rename label field to match HF expected key\n",
    "sst2_val = sst2_val.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# convert dataset into torch tensors + dataloader\n",
    "sst2_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "val_dataloader = DataLoader(sst2_val, batch_size=32, shuffle=False)\n",
    "\n",
    "# label mapping reference\n",
    "sst2_labels = [0, 1]  # 0 = negative, 1 = positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e840fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# function to generate predictions for entire dataloader\n",
    "@torch.no_grad()\n",
    "def get_all_predictions(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    print(\"Gathering all predictions for confusion matrix...\")\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        # move tensors to gpu/cpu device\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # get class with max logit\n",
    "        pred = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "        # store predictions + labels\n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer import\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load + prepare SST-2 training set\n",
    "sst2_train = dataset['train'].map(tokenize_sst2, batched=True)\n",
    "sst2_train = sst2_train.rename_column(\"label\", \"labels\")   # match eval naming\n",
    "sst2_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "train_dataloader = DataLoader(sst2_train, batch_size=16, shuffle=True)\n",
    "\n",
    "# configure optimizer + training mode\n",
    "model_bert_fp32_clf.train()\n",
    "optimizer = AdamW(model_bert_fp32_clf.parameters(), lr=2e-5)\n",
    "num_epochs = 2\n",
    "\n",
    "print(\"--- Fine-Tuning FP32 BERT on SST-2 ---\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting Epoch {epoch + 1}/{num_epochs}\")\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k: v.to(device) for k,v in batch.items()}\n",
    "\n",
    "        # forward + loss\n",
    "        outputs = model_bert_fp32_clf(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # epoch summary print\n",
    "    print(f\"Epoch {epoch + 1} complete. Loss: {loss.item()}\")\n",
    "\n",
    "print(\"--- Fine-Tuning Complete ---\")\n",
    "model_bert_fp32_clf.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1302c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for dataset subsets and accuracy evaluation\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# create quantization functions for adaptive float\n",
    "quant_af4_func = partial(quantizer.quantize_to_adaptivfloat, total_bits=4, exponent_bits=2)\n",
    "quant_af4_func.__name__ = \"AdaptivFloat_4bit\"\n",
    "\n",
    "quant_af8_func = partial(quantizer.quantize_to_adaptivfloat, total_bits=8, exponent_bits=3)\n",
    "quant_af8_func.__name__ = \"AdaptivFloat_8bit\"\n",
    "\n",
    "# create a smaller \"quick\" dataloader for faster evaluation\n",
    "val_dataset = val_dataloader.dataset\n",
    "subset_indices = list(range(0, len(val_dataset), 5)) # use 1/5th = 20% of validation set\n",
    "quick_dataset = Subset(val_dataset, subset_indices)\n",
    "quick_loader = DataLoader(quick_dataset, batch_size=val_dataloader.batch_size)\n",
    "print(f\"Using full validation set of {len(val_dataset)} samples.\")\n",
    "print(f\"Using quick analysis subset of {len(quick_dataset)} samples.\")\n",
    "\n",
    "# helper function to compute accuracy on a dataloader\n",
    "@torch.no_grad()\n",
    "def get_accuracy(model, dataloader, device):\n",
    "    \"\"\"Helper function to get just the accuracy number.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        if isinstance(batch, list):\n",
    "            data, labels = batch\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            model_input = data \n",
    "        elif isinstance(batch, dict):\n",
    "            labels = batch.pop('labels').to(device)\n",
    "            model_input = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        if isinstance(model_input, dict):\n",
    "            outputs = model(**model_input)\n",
    "        else:\n",
    "            outputs = model(model_input)\n",
    "        \n",
    "        if isinstance(outputs, torch.Tensor):\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "        else:\n",
    "            pred = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "        all_preds.extend(pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    return accuracy_score(all_labels, all_preds)\n",
    "\n",
    "# calculate baseline FP32 accuracy on the quick subset\n",
    "print(\"Calculating baseline FP32 accuracy on quick loader...\")\n",
    "baseline_quick_accuracy = get_accuracy(model_bert_fp32_clf, quick_loader, device)\n",
    "print(f\"Baseline FP32 (Quick): {baseline_quick_accuracy * 100:.2f}%\")\n",
    "\n",
    "# collect all weight layer names in the model\n",
    "layer_names = [name for name, param in model_bert_fp32_clf.named_parameters() if 'weight' in name]\n",
    "print(f\"Found {len(layer_names)} layers with weights to analyze.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d54f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling, JSON, copy, progress bar, and torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "sensitivity_results = {}\n",
    "baseline_acc = baseline_quick_accuracy\n",
    "\n",
    "print(f\"\\n--- Starting Sensitivity Analysis (Quantizing 1 layer at a time to INT4) ---\")\n",
    "\n",
    "# loop through each layer to test its sensitivity to INT4 quantization\n",
    "for layer_to_quantize in tqdm(layer_names, desc=\"Analyzing Layers\"):\n",
    "    \n",
    "    # create a fresh deep copy of the FP32 model\n",
    "    temp_model = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "    \n",
    "    # find the target layer and apply INT4 quantization\n",
    "    found = False\n",
    "    for name, param in temp_model.named_parameters():\n",
    "        if name == layer_to_quantize:\n",
    "            param.data.copy_(quant_af4_func(param.data))\n",
    "            found = True\n",
    "            break # no need to check other layers\n",
    "    \n",
    "    if not found:\n",
    "        print(f\"Warning: Layer {layer_to_quantize} not found in model params.\")\n",
    "        continue\n",
    "\n",
    "    # evaluate the model with only this layer quantized\n",
    "    current_accuracy = get_accuracy(temp_model, quick_loader, device)\n",
    "    \n",
    "    # compute and log accuracy drop\n",
    "    accuracy_drop = baseline_acc - current_accuracy\n",
    "    sensitivity_results[layer_to_quantize] = accuracy_drop\n",
    "    \n",
    "    # clean up GPU memory\n",
    "    del temp_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"--- Analysis Complete ---\")\n",
    "\n",
    "# convert results to a dataframe and sort by accuracy drop\n",
    "sensitivity_df = pd.DataFrame(\n",
    "    list(sensitivity_results.items()), \n",
    "    columns=['Layer', 'Accuracy Drop']\n",
    ").sort_values(by='Accuracy Drop', ascending=False)\n",
    "\n",
    "# display most sensitive layers (largest drop)\n",
    "print(\"\\nMost Sensitive Layers (Largest Accuracy Drop when set to INT4):\")\n",
    "print(sensitivity_df.head(10))\n",
    "\n",
    "# display most robust layers (smallest drop)\n",
    "print(\"\\nMost Robust Layers (Smallest Accuracy Drop when set to INT4):\")\n",
    "print(sensitivity_df.tail(10))\n",
    "\n",
    "# optionally save results to CSV\n",
    "sensitivity_df.to_csv(\"sensitivity_analysis_results.csv\", index=False)\n",
    "print(\"\\nFull sensitivity results saved to 'sensitivity_analysis_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling and JSON\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "sensitivity_df = pd.read_csv(\"sensitivity_analysis_results.csv\")\n",
    "print(f\"Loaded sensitivity results for {len(sensitivity_df)} layers.\")\n",
    "\n",
    "# define threshold to distinguish sensitive layers (1% drop)\n",
    "SENSITIVITY_THRESHOLD = 0.01\n",
    "\n",
    "precision_profile = {}\n",
    "num_sensitive_layers = 0\n",
    "\n",
    "# create layer precision profile based on sensitivity\n",
    "for index, row in sensitivity_df.iterrows():\n",
    "    layer_name = row['Layer']\n",
    "    accuracy_drop = row['Accuracy Drop']\n",
    "    \n",
    "    if accuracy_drop > SENSITIVITY_THRESHOLD:\n",
    "        precision_profile[layer_name] = 'INT8'\n",
    "        num_sensitive_layers += 1\n",
    "    else:\n",
    "        precision_profile[layer_name] = 'INT4'\n",
    "\n",
    "print(f\"\\nCreated profile with {num_sensitive_layers} layers at INT8\")\n",
    "print(f\"and {len(precision_profile) - num_sensitive_layers} layers at INT4.\")\n",
    "\n",
    "# save the precision profile to JSON for later use\n",
    "with open('bert_precision_profile.json', 'w') as f:\n",
    "    json.dump(precision_profile, f, indent=4)\n",
    "\n",
    "print(\"Precision profile saved to 'bert_precision_profile.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f920ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for evaluation metrics and plotting\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# helper function to plot a confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using scikit-learn.\n",
    "    \"\"\"\n",
    "    # compute confusion matrix\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    # create display object for plotting\n",
    "    disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "    # setup figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    \n",
    "    # plot confusion matrix with blue color map\n",
    "    disp.plot(ax=ax, cmap='Blues')\n",
    "    \n",
    "    # set plot title and show\n",
    "    plt.title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056dd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for JSON handling and deep copy\n",
    "import json\n",
    "import copy\n",
    "\n",
    "with open('bert_precision_profile.json', 'r') as f:\n",
    "    precision_profile = json.load(f)\n",
    "print(f\"Loaded precision profile with {len(precision_profile)} layers.\")\n",
    "\n",
    "# ensure required variables and utils are loaded\n",
    "# model_bert_fp32_clf, val_dataloader, sst2_labels, utils.apply_quantization_to_model\n",
    "\n",
    "# Test 1: FP32 Baseline evaluation on full dataset\n",
    "print(\"\\n--- 1. Evaluating FP32 Baseline (Full Dataset) ---\")\n",
    "y_true, y_pred_fp32 = get_all_predictions(model_bert_fp32_clf, val_dataloader, device)\n",
    "acc_fp32 = accuracy_score(y_true, y_pred_fp32)\n",
    "plot_confusion_matrix(y_true, y_pred_fp32, sst2_labels, f\"BERT - FP32 Baseline (Acc: {acc_fp32*100:.2f}%)\")\n",
    "\n",
    "# Test 2: Uniform INT8 quantization\n",
    "print(\"\\n--- 2. Evaluating Uniform INT8 ---\")\n",
    "model_int8 = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_int8, quant_af8_func) # apply 8-bit quant function\n",
    "y_true, y_pred_int8 = get_all_predictions(model_int8, val_dataloader, device)\n",
    "acc_int8 = accuracy_score(y_true, y_pred_int8)\n",
    "plot_confusion_matrix(y_true, y_pred_int8, sst2_labels, f\"BERT - Uniform INT8 (Acc: {acc_int8*100:.2f}%)\")\n",
    "\n",
    "# Test 3: Uniform INT4 quantization (expected poor performance)\n",
    "print(\"\\n--- 3. Evaluating Uniform INT4 ---\")\n",
    "model_int4 = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_int4, quant_af4_func) # apply 4-bit quant function\n",
    "y_true, y_pred_int4 = get_all_predictions(model_int4, val_dataloader, device)\n",
    "acc_int4 = accuracy_score(y_true, y_pred_int4)\n",
    "plot_confusion_matrix(y_true, y_pred_int4, sst2_labels, f\"BERT - Uniform INT4 (Acc: {acc_int4*100:.2f}%)\")\n",
    "\n",
    "# Test 4: Mixed-Precision INT4/INT8 according to profile\n",
    "print(\"\\n--- 4. Evaluating MIXED PRECISION (INT4/INT8) ---\")\n",
    "model_mixed = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_mixed, precision_profile) # apply dictionary-based quantization\n",
    "y_true, y_pred_mixed = get_all_predictions(model_mixed, val_dataloader, device)\n",
    "acc_mixed = accuracy_score(y_true, y_pred_mixed)\n",
    "plot_confusion_matrix(y_true, y_pred_mixed, sst2_labels, f\"BERT - Mixed-Precision (Acc: {acc_mixed*100:.2f}%)\")\n",
    "\n",
    "# Final Accuracy Report\n",
    "print(\"\\n--- Final Accuracy Report ---\")\n",
    "print(f\"FP32 Baseline:       {acc_fp32 * 100:.2f}%\")\n",
    "print(f\"Uniform INT8:        {acc_int8 * 100:.2f}%\")\n",
    "print(f\"Uniform INT4:        {acc_int4 * 100:.2f}%\")\n",
    "print(f\"Mixed-Precision (New): {acc_mixed * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace030b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy module\n",
    "import copy\n",
    "\n",
    "# create a uniform INT8 quantized model\n",
    "model_int8_baseline = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_int8_baseline, quant_af8_func)\n",
    "\n",
    "# evaluate accuracy on the quick validation subset\n",
    "print(\"Calculating baseline Uniform INT8 accuracy on quick loader...\")\n",
    "baseline_int8_accuracy = get_accuracy(model_int8_baseline, quick_loader, device)\n",
    "print(f\"Baseline Uniform INT8 (Quick): {baseline_int8_accuracy * 100:.2f}%\")\n",
    "\n",
    "del model_int8_baseline\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a43c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling, JSON, copy, progress bar, and torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# initialize dictionary to store sensitivity results\n",
    "new_sensitivity_results = {}\n",
    "baseline_acc = baseline_int8_accuracy # baseline is now the INT8 model\n",
    "\n",
    "print(f\"\\n--- Starting New Analysis (Demoting 1 layer at a time from INT8 to INT4) ---\")\n",
    "\n",
    "# loop through each layer to demote it from INT8 to INT4 and measure impact\n",
    "for layer_to_demote in tqdm(layer_names, desc=\"Analyzing Layers (INT8->INT4)\"):\n",
    "    \n",
    "    # create a fresh uniform INT8 model\n",
    "    temp_model = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "    utils.apply_quantization_to_model(temp_model, quant_af8_func)\n",
    "    \n",
    "    # find and demote only the target layer to INT4\n",
    "    found = False\n",
    "    for name, param in temp_model.named_parameters():\n",
    "        if name == layer_to_demote:\n",
    "            param.data.copy_(quant_af4_func(param.data)) # apply INT4\n",
    "            found = True\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        print(f\"Warning: Layer {layer_to_demote} not found.\")\n",
    "        continue\n",
    "\n",
    "    # evaluate this mostly INT8 model\n",
    "    current_accuracy = get_accuracy(temp_model, quick_loader, device)\n",
    "    \n",
    "    # log the accuracy drop relative to INT8 baseline\n",
    "    accuracy_drop = baseline_acc - current_accuracy\n",
    "    new_sensitivity_results[layer_to_demote] = accuracy_drop\n",
    "    \n",
    "    # clean up GPU memory\n",
    "    del temp_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"--- New Analysis Complete ---\")\n",
    "\n",
    "# convert results to dataframe and sort by accuracy drop ascending\n",
    "new_sensitivity_df = pd.DataFrame(\n",
    "    list(new_sensitivity_results.items()), \n",
    "    columns=['Layer', 'Accuracy Drop']\n",
    ").sort_values(by='Accuracy Drop', ascending=True) # ascending: smallest drop first\n",
    "\n",
    "# display most robust layers (safe to demote to INT4)\n",
    "print(\"\\nMost Robust Layers (Safest to demote to INT4):\")\n",
    "print(new_sensitivity_df.head(20))\n",
    "\n",
    "# display most sensitive layers (keep these at INT8)\n",
    "print(\"\\nMost Sensitive Layers (Keep these at INT8):\")\n",
    "print(new_sensitivity_df.tail(10))\n",
    "\n",
    "# save new sensitivity results to CSV\n",
    "new_sensitivity_df.to_csv(\"sensitivity_analysis_results_v2.csv\", index=False)\n",
    "print(\"\\nFull V2 sensitivity results saved to 'sensitivity_analysis_results_v2.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2bd2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling and JSON\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load the new V2 sensitivity results\n",
    "new_sensitivity_df = pd.read_csv(\"sensitivity_analysis_results_v2.csv\")\n",
    "print(f\"Loaded new V2 sensitivity results for {len(new_sensitivity_df)} layers.\")\n",
    "\n",
    "# define threshold for safe demotion to INT4 (0.1% accuracy drop)\n",
    "DEMOTION_THRESHOLD = 0.001 # 0.1% drop\n",
    "\n",
    "# initialize new precision profile\n",
    "new_precision_profile = {}\n",
    "num_int4_layers = 0\n",
    "\n",
    "# assign INT4 or INT8 to each layer based on threshold\n",
    "for index, row in new_sensitivity_df.iterrows():\n",
    "    layer_name = row['Layer']\n",
    "    accuracy_drop = row['Accuracy Drop']\n",
    "    \n",
    "    if accuracy_drop <= DEMOTION_THRESHOLD:\n",
    "        new_precision_profile[layer_name] = 'INT4'\n",
    "        num_int4_layers += 1\n",
    "    else:\n",
    "        new_precision_profile[layer_name] = 'INT8'\n",
    "\n",
    "print(f\"\\nCreated new profile with {num_int4_layers} layers at INT4\")\n",
    "print(f\"and {len(new_precision_profile) - num_int4_layers} layers at INT8.\")\n",
    "\n",
    "# save the new V2 precision profile to JSON\n",
    "with open('bert_precision_profile_v2.json', 'w') as f:\n",
    "    json.dump(new_precision_profile, f, indent=4)\n",
    "\n",
    "print(\"New V2 precision profile saved to 'bert_precision_profile_v2.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64c0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for JSON handling and deep copy\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# load the new V2 precision profile\n",
    "with open('bert_precision_profile_v2.json', 'r') as f:\n",
    "    new_precision_profile = json.load(f)\n",
    "print(f\"Loaded V2 precision profile with {len(new_precision_profile)} layers.\")\n",
    "\n",
    "# Test 1: FP32 Baseline evaluation on full dataset\n",
    "print(\"\\n--- 1. Evaluating FP32 Baseline (Full Dataset) ---\")\n",
    "y_true_fp32, y_pred_fp32 = get_all_predictions(model_bert_fp32_clf, val_dataloader, device)\n",
    "acc_fp32 = accuracy_score(y_true_fp32, y_pred_fp32)\n",
    "plot_confusion_matrix(y_true_fp32, y_pred_fp32, sst2_labels, f\"BERT - FP32 Baseline (Acc: {acc_fp32*100:.2f}%)\")\n",
    "\n",
    "# Test 2: Uniform INT8 quantization\n",
    "print(\"\\n--- 2. Evaluating Uniform INT8 ---\")\n",
    "model_int8 = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_int8, quant_af8_func) # apply 8-bit quant function\n",
    "y_true_int8, y_pred_int8 = get_all_predictions(model_int8, val_dataloader, device)\n",
    "acc_int8 = accuracy_score(y_true_int8, y_pred_int8)\n",
    "plot_confusion_matrix(y_true_int8, y_pred_int8, sst2_labels, f\"BERT - Uniform INT8 (Acc: {acc_int8*100:.2f}%)\")\n",
    "\n",
    "# Test 3: Uniform INT4 quantization (expected poor performance)\n",
    "print(\"\\n--- 3. Evaluating Uniform INT4 ---\")\n",
    "model_int4 = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_int4, quant_af4_func) # apply 4-bit quant function\n",
    "y_true_int4, y_pred_int4 = get_all_predictions(model_int4, val_dataloader, device)\n",
    "acc_int4 = accuracy_score(y_true_int4, y_pred_int4)\n",
    "plot_confusion_matrix(y_true_int4, y_pred_int4, sst2_labels, f\"BERT - Uniform INT4 (Acc: {acc_int4*100:.2f}%)\")\n",
    "\n",
    "# Test 4: Mixed-Precision V2 according to new profile\n",
    "print(\"\\n--- 4. Evaluating NEW Mixed-Precision (V2) ---\")\n",
    "model_mixed_v2 = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_mixed_v2, new_precision_profile) # apply V2 profile\n",
    "y_true_mixed_v2, y_pred_mixed_v2 = get_all_predictions(model_mixed_v2, val_dataloader, device)\n",
    "acc_mixed_v2 = accuracy_score(y_true_mixed_v2, y_pred_mixed_v2)\n",
    "plot_confusion_matrix(y_true_mixed_v2, y_pred_mixed_v2, sst2_labels, f\"BERT - Mixed-Precision V2 (Acc: {acc_mixed_v2*100:.2f}%)\")\n",
    "\n",
    "# Final Accuracy Report (V2)\n",
    "print(\"\\n--- Final Accuracy Report (V2) ---\")\n",
    "print(f\"FP32 Baseline:         {acc_fp32 * 100:.2f}%\")\n",
    "print(f\"Uniform INT8:          {acc_int8 * 100:.2f}%\")\n",
    "print(f\"Uniform INT4:          {acc_int4 * 100:.2f}%\")\n",
    "print(f\"Mixed-Precision (V2):  {acc_mixed_v2 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b7c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling, JSON, copy, progress bar, and torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# --- 1. Load Initial V2 Sensitivity Results ---\n",
    "# Used to get the first layer to demote\n",
    "try:\n",
    "    new_sensitivity_df = pd.read_csv(\"sensitivity_analysis_results_v2.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'sensitivity_analysis_results_v2.csv' not found.\")\n",
    "    print(\"Please re-run the 'New Cell 2: Run the INT8 -> INT4 Demotion Analysis' first.\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Setup the Greedy Search ---\n",
    "print(\"--- Starting Greedy Iterative Mixed-Precision Search ---\")\n",
    "ACCURACY_TARGET = 0.91 # stop condition (91% accuracy)\n",
    "baseline_acc = baseline_int8_accuracy # baseline from INT8 model\n",
    "\n",
    "# start with all layers at INT8\n",
    "final_precision_profile = {layer: 'INT8' for layer in layer_names}\n",
    "# layers still available to be demoted\n",
    "layers_to_test = set(layer_names)\n",
    "\n",
    "# history of search results\n",
    "search_history = [] \n",
    "\n",
    "# iterate over all layers\n",
    "for i in range(len(layer_names)):\n",
    "    print(f\"\\n--- Iteration {i+1} / {len(layer_names)} ---\")\n",
    "    \n",
    "    # 1. Build current baseline model with existing profile\n",
    "    current_baseline_model = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "    utils.apply_quantization_to_model(current_baseline_model, final_precision_profile)\n",
    "    \n",
    "    # 2. Get its accuracy on quick loader\n",
    "    current_baseline_acc = get_accuracy(current_baseline_model, quick_loader, device)\n",
    "    print(f\"Current baseline accuracy: {current_baseline_acc * 100:.2f}%\")\n",
    "    search_history.append((i, current_baseline_acc)) # log the result\n",
    "    \n",
    "    # 3. Check stop condition\n",
    "    if current_baseline_acc < ACCURACY_TARGET:\n",
    "        print(f\"Accuracy dropped below {ACCURACY_TARGET*100}%. Stopping search.\")\n",
    "        # revert the last change\n",
    "        final_precision_profile[last_layer_demoted] = 'INT8'\n",
    "        search_history.pop() # remove failing entry\n",
    "        break\n",
    "    \n",
    "    if not layers_to_test:\n",
    "        print(\"All layers have been demoted to INT4.\")\n",
    "        break\n",
    "\n",
    "    # 4. Find the next best layer to demote\n",
    "    best_layer_to_demote = None\n",
    "    best_layer_accuracy = -1.0\n",
    "    \n",
    "    # iterate over layers still at INT8\n",
    "    for layer_name in tqdm(list(layers_to_test), desc=\"Finding next best layer\"):\n",
    "        \n",
    "        # create test profile with one additional INT4 layer\n",
    "        test_profile = final_precision_profile.copy()\n",
    "        test_profile[layer_name] = 'INT4'\n",
    "        \n",
    "        # build test model\n",
    "        test_model = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "        utils.apply_quantization_to_model(test_model, test_profile)\n",
    "        \n",
    "        # evaluate accuracy\n",
    "        acc = get_accuracy(test_model, quick_loader, device)\n",
    "        \n",
    "        # track the best candidate\n",
    "        if acc > best_layer_accuracy:\n",
    "            best_layer_accuracy = acc\n",
    "            best_layer_to_demote = layer_name\n",
    "            \n",
    "        # clean up memory\n",
    "        del test_model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # 5. Lock in the best layer\n",
    "    final_precision_profile[best_layer_to_demote] = 'INT4'\n",
    "    layers_to_test.remove(best_layer_to_demote)\n",
    "    last_layer_demoted = best_layer_to_demote\n",
    "    \n",
    "    print(f\"Demoting layer: {best_layer_to_demote} (New Acc: {best_layer_accuracy * 100:.2f}%)\")\n",
    "    del current_baseline_model\n",
    "\n",
    "print(\"--- Greedy Search Complete ---\")\n",
    "\n",
    "# --- 6. Save the new, optimal profile ---\n",
    "num_int4 = list(final_precision_profile.values()).count('INT4')\n",
    "num_int8 = len(final_precision_profile) - num_int4\n",
    "print(f\"Created OPTIMAL profile with {num_int4} layers at INT4 and {num_int8} at INT8\")\n",
    "\n",
    "with open('bert_precision_profile_OPTIMAL.json', 'w') as f:\n",
    "    json.dump(final_precision_profile, f, indent=4)\n",
    "print(\"Optimal profile saved to 'bert_precision_profile_OPTIMAL.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782233b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for JSON handling and deep copy\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# --- 1. Load the NEW OPTIMAL Profile ---\n",
    "with open('bert_precision_profile_OPTIMAL.json', 'r') as f:\n",
    "    optimal_profile = json.load(f)\n",
    "print(f\"Loaded OPTIMAL profile with {len(optimal_profile)} layers.\")\n",
    "\n",
    "# --- 2. Evaluate the New Optimal Mixed-Precision Model ---\n",
    "print(\"\\n--- Evaluating OPTIMAL Mixed-Precision ---\")\n",
    "model_optimal = copy.deepcopy(model_bert_fp32_clf).to(device)\n",
    "utils.apply_quantization_to_model(model_optimal, optimal_profile) # apply OPTIMAL profile\n",
    "y_true_optimal, y_pred_optimal = get_all_predictions(model_optimal, val_dataloader, device)\n",
    "acc_optimal = accuracy_score(y_true_optimal, y_pred_optimal)\n",
    "plot_confusion_matrix(y_true_optimal, y_pred_optimal, sst2_labels, f\"BERT - Optimal Mixed-Precision (Acc: {acc_optimal*100:.2f}%)\")\n",
    "\n",
    "# --- 3. Final Accuracy Report ---\n",
    "print(\"\\n--- Final Accuracy Report (OPTIMAL) ---\")\n",
    "print(f\"FP32 Baseline:         {acc_fp32 * 100:.2f}%\") # from previous run\n",
    "print(f\"Uniform INT8:          {acc_int8 * 100:.2f}%\") # from previous run\n",
    "print(f\"Uniform INT4:          {acc_int4 * 100:.2f}%\") # from previous run\n",
    "print(f\"Optimal Mixed-Precision: {acc_optimal * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
