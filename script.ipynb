{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6VWsH61aNs+Nuihm5e68Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarutkarsh99/AdaptivFloat-Implementation/blob/main/script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCdeU_9UgCU2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from functools import partial\n",
        "\n",
        "# Import our custom files\n",
        "import quantizer\n",
        "import utils\n",
        "\n",
        "# Set up the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "nPi8Cz3zgPMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# A simple CNN model designed for MNIST (1 input channel)\n",
        "class MnistCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1) # 1 input channel\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128) # 64 * 12 * 12 = 9216\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 output classes for digits 0-9\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "3kEVBy2OhAgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transforms for MNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Standard MNIST mean/std\n",
        "    ])\n",
        "\n",
        "# Download and load the datasets\n",
        "trainset = datasets.MNIST('./data', train=True, download=True,\n",
        "                   transform=transform)\n",
        "testset = datasets.MNIST('./data', train=False,\n",
        "                   transform=transform)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "id": "2AXUe9dRhE2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_fp32 = MnistCNN().to(device)\n",
        "optimizer = optim.Adam(model_fp32.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "num_epochs = 10 # More epochs for high accuracy\n",
        "\n",
        "print(\"Training a new FP32 baseline model on MNIST...\")\n",
        "model_fp32.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for data, target in tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model_fp32(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} complete.\")\n",
        "print(\"Finished Training.\")"
      ],
      "metadata": {
        "id": "g7Ldd3Idko4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "# --- 1. Evaluate our new FP32 baseline ---\n",
        "# We reuse the `evaluate_resnet` function, but change the printout\n",
        "fp32_accuracy = utils.evaluate_resnet(model_fp32, testloader, device)\n",
        "\n",
        "# --- 2. Create a deep copy for quantization ---\n",
        "model_af_8bit = copy.deepcopy(model_fp32).to(device)\n",
        "\n",
        "# --- 3. Define and apply 8-bit AdaptivFloat ---\n",
        "quant_8bit_af_func = partial(quantizer.quantize_to_adaptivfloat,\n",
        "                             total_bits=8,\n",
        "                             exponent_bits=3)\n",
        "quant_8bit_af_func.__name__ = \"AdaptivFloat_8bit\"\n",
        "utils.apply_quantization_to_model(model_af_8bit, quant_8bit_af_func)\n",
        "\n",
        "# --- 4. Evaluate the quantized model ---\n",
        "af_8bit_accuracy = utils.evaluate_resnet(model_af_8bit, testloader, device)\n",
        "\n",
        "# --- 5. Print the final, report-ready comparison ---\n",
        "print(\"\\n--- MNIST Sanity Check Complete ---\")\n",
        "print(f\"Baseline FP32 Accuracy (MnistCNN):   {fp32_accuracy:.2f}%\")\n",
        "print(f\"AdaptivFloat 8-bit Accuracy (MnistCNN): {af_8bit_accuracy:.2f}%\")\n",
        "print(f\"Accuracy Drop: {fp32_accuracy - af_8bit_accuracy:.4f}%\")"
      ],
      "metadata": {
        "id": "07CWTEX9lQfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# Load BERT tokenizer and model\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# The test sentence. The correct answer is \"paris\"\n",
        "sentence = \"The capital of France is [MASK].\""
      ],
      "metadata": {
        "id": "cohVp8-dhJ7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a fresh FP32 model\n",
        "model_bert_fp32 = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Run the evaluation\n",
        "predicted_token = utils.evaluate_bert(model_bert_fp32, tokenizer, sentence, device)\n",
        "print(f\"\\n[FP32 Baseline BERT] Prediction for '[MASK]': '{predicted_token}'\")"
      ],
      "metadata": {
        "id": "by8aujzchO-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a fresh model\n",
        "model_bert_int8 = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Create the \"bad\" INT8 quantizer function\n",
        "quant_int8_func = partial(utils.simple_int8_quantizer, total_bits=8)\n",
        "quant_int8_func.__name__ = \"Simple_INT8\"\n",
        "\n",
        "# Apply this bad quantization\n",
        "utils.apply_quantization_to_model(model_bert_int8, quant_int8_func)\n",
        "\n",
        "# Evaluate\n",
        "predicted_token_int8 = utils.evaluate_bert(model_bert_int8, tokenizer, sentence, device)\n",
        "print(f\"\\n[Simple INT8 BERT] Prediction for '[MASK]': '{predicted_token_int8}'\")"
      ],
      "metadata": {
        "id": "pHgSnkCfhVy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a fresh model\n",
        "model_bert_af8 = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Apply our GOOD AdaptivFloat 8-bit quantizer\n",
        "# We re-use the function from the ResNet test\n",
        "utils.apply_quantization_to_model(model_bert_af8, quant_8bit_af_func)\n",
        "\n",
        "# Evaluate\n",
        "predicted_token_af8 = utils.evaluate_bert(model_bert_af8, tokenizer, sentence, device)\n",
        "print(f\"\\n[AdaptivFloat 8-bit BERT] Prediction for '[MASK]': '{predicted_token_af8}'\")"
      ],
      "metadata": {
        "id": "2ydRcDSuhZu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Advanced Test: Going to lower bit-widths ---\")\n",
        "\n",
        "# --- 6-bit (3 exp bits) ---\n",
        "model_bert_af6 = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "quant_6bit_af_func = partial(quantizer.quantize_to_adaptivfloat, total_bits=6, exponent_bits=3)\n",
        "quant_6bit_af_func.__name__ = \"AdaptivFloat_6bit_3exp\"\n",
        "utils.apply_quantization_to_model(model_bert_af6, quant_6bit_af_func)\n",
        "predicted_token_af6 = utils.evaluate_bert(model_bert_af6, tokenizer, sentence, device)\n",
        "print(f\"[AdaptivFloat 6-bit BERT] Prediction for '[MASK]': '{predicted_token_af6}'\")\n",
        "\n",
        "# --- 4-bit (2 exp bits) ---\n",
        "model_bert_af4 = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n",
        "quant_4bit_af_func = partial(quantizer.quantize_to_adaptivfloat, total_bits=4, exponent_bits=2)\n",
        "quant_4bit_af_func.__name__ = \"AdaptivFloat_4bit_2exp\"\n",
        "utils.apply_quantization_to_model(model_bert_af4, quant_4bit_af_func)\n",
        "predicted_token_af4 = utils.evaluate_bert(model_bert_af4, tokenizer, sentence, device)\n",
        "print(f\"[AdaptivFloat 4-bit BERT] Prediction for '[MASK]': '{predicted_token_af4}'\")"
      ],
      "metadata": {
        "id": "aANb39S0hc0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "krWvKLj6hfyN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}